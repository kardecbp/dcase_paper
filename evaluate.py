import numpy as np 
np.random.seed(1001)
import pandas as pd 
import os
import pickle
from metrics import mapk, apk


dir_path = os.path.dirname(os.path.realpath(__file__))
data_dir = os.path.join(dir_path, 'data')
train_path = os.path.join(data_dir, 'audio_train')
test_path = os.path.join(data_dir, 'audio_test')
prediction_path = os.path.join(dir_path, 'predictions')

audio_train_files = os.listdir(train_path)
audio_test_files = os.listdir(test_path)
train = pd.read_csv(os.path.join(data_dir, 'train.csv'))
test = pd.read_csv(os.path.join(data_dir, "sample_submission.csv"))
train_post = train = pd.read_csv(os.path.join(data_dir, 'train_post_competition.csv'))
test_post = pd.read_csv(os.path.join(data_dir, "test_post_competition.csv"))

LABELS = list(train.label.unique())
label_idx = {label: i for i, label in enumerate(LABELS)}
train["label_idx"] = train.label.apply(lambda x: label_idx[x])
test_post = test_post[test_post['label'] != 'None']
test_post['label_idx'] = test_post.label.apply(lambda x: label_idx[x])
true_ids = list(test_post.index)
ids_private = list(test_post[test_post['usage'] == 'Private'].index)
target_test_private = list(test_post[test_post['usage'] == 'Private']['label_idx'])
ids_public = list(test_post[test_post['usage'] == 'Public'].index)
target_test_public = list(test_post[test_post['usage'] == 'Public']['label_idx'])
target_test = list(test_post['label_idx'])


def calc_mapk(preds, labels):
	preds_top3 = [list(np.argsort(preds[i])[::-1][:3]) for i in range(len(preds))]
	actual = [[label] for label in labels]
	score = mapk(actual, preds_top3, k=3)
	return score


def evaluate_model(file_name, ensemble=True):
	'''
	file_name - name of the pickle file with predictions
	ensemble - boolean variable indicating if predictions are generated by stacking or a single model
	'''
	if ensemble:
		final_preds = pickle.load(open(os.path.join(prediction_path, 'ensembles', file_name), 'rb'))
	else:
		final_preds = pickle.load(open(os.path.join(prediction_path, 'final', file_name), 'rb'))
		final_preds = np.array(final_preds)

	final_preds_true = final_preds[true_ids]
	
	print('Test score on public test data: ' + str(calc_mapk(final_preds[ids_public], target_test_public)))
	print('Test score on private test data: ' + str(calc_mapk(final_preds[ids_private], target_test_private)))
	print('Test score on the entire testing data: ' + str(calc_mapk(final_preds_true, target_test)))
	print('--------------------')
	print('Score and number of occurrences per class: ')

	class_dict = {}
	for i in range(len(target_test)):
		preds_top3 = list(np.argsort(final_preds_true[i])[::-1][:3])
		score = apk([target_test[i]], preds_top3)
		if target_test[i] not in class_dict:
			class_dict[target_test[i]] = [score, 1]
		else:
			class_dict[target_test[i]][0] = score + class_dict[target_test[i]][0]
			class_dict[target_test[i]][1] = 1 + class_dict[target_test[i]][1]
	ap3_per_class = {key: class_dict[key][0] / class_dict[key][1] for key in class_dict}
	for key in ap3_per_class:
		print(LABELS[key], ap3_per_class[key], class_dict[key][1])
	

if __name__ == '__main__':
	evaluate_model('preds_test_model5_cqt_4s.pickle', False)